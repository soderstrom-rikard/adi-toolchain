#!/bin/sh
#
# toolchain-regtest
#
# Do regression testing on installed toolchain
#
# This file is not an official part of GCC.
#
# This is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2, or (at your option)
# any later version.
#
# This is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with GCC; see the file COPYING.  If not, write to
# the Free Software Foundation, 59 Temple Place - Suite 330,
# Boston, MA 02111-1307, USA.
#
###############################################################

. "${0%/*}/lib/lib.sh" || exit 1

START=$(date +%s)

#Uncomment only one of these
#TIMESTAMP=`date +%Y-%b-%d-%H-%M` # is YYYY_MMM_DD_HH_MM
TIMESTAMP=`date +%Y%m%d`       # is YYYY_MMM_DD

PREREQ_FILE=""
PREREQ_TESTS="bc kill killall pstree"
PREREQ_HW_TESTS="ping rsh rcp"
RUNTEST=runtest
BINUTILS_VERSION=
GDB_VERSION=
GCC_VERSION=
BINUTILS_PATH=
GDB_PATH=
GCC_PATH=

VALID_TEST_TARGETS="
	elf-sim elf-jtag
	uclinux uclinux-qemu
	linux-uclibc linux-uclibc-sim linux-uclibc-qemu
"
TEST_TARGETS=
_test_targets()
{
	case $1 in
		LINUX) echo uclinux linux-uclibc ;;
		*)     echo "$@" ;;
	esac
}
test_targets_queue() { TEST_TARGETS="${TEST_TARGETS} `_test_targets $*`" ; }
test_targets_has()
{
	local t i=1
	for t in `_test_targets "$@"` ; do
		has ${t} ${TEST_TARGETS} && i=0
	done
	return $i
}

VALID_TESTS="
	binutils gas ld sim gdb newlib
	gcc g++ gfortran objc libstdc++ libmudflap libgomp
"

usage() {
# We don't document the -S option - it's for advanced users who know what
# they are doing.  For maximum reproducibility, not using it is better.
cat <<-EOF
Usage : $APP_NAME -s SOURCE [OPTIONS] [[!]TEST[=/path/to/testsuite]] [...]
-a               Add --all to runtest
-b BUILD         Build directory (leave blank to use current dir)
-d               Date Stamp all output files
-f               Add stack checking to all tests
-h               Help
-H HOST          ??? Build toolchain to run on HOST.
-L               Save test result files in legacy name
-O OUTPUT        Output directory (toolchain to be tested)
-o OUTPUT        Output directory prefix (toolchain to be tested)
-p               Check prerequisites
-R RUNTEST       Default runtest (leave blank to use system runtest)
-r RESULTS_DIR   Compare current tests to old results
-s SOURCE        Toolchain Source directory (where to find testsuites)
-T TARGET_IPADDR Test components on hardware
-t TARGET        Test target (`echo ${VALID_TEST_TARGETS}`)
-u SOURCE        Uboot Source directory
-v               Verbose
-w               Warnings to be sent to a different log file

Possible TESTS: `echo ${VALID_TESTS}`
EOF
exit ${1:-0}
}

while getopts ab:B:cdfH:hLo:O:pr:R:s:St:T:U:vw parameter
do
    case $parameter in
        a)  RUNTEST_FLAGS=-all ;;
        p)  CHECK_PRE=true ;;
        c)  CHECK_LIB=true ;;
        R)  RUNTEST=$OPTARG ;;
        w)  WARN=1 ;;
        d)  DATE_STAMP=1 ;;
        v)  VERBOSE=1 ;;
        t)
            if has ${OPTARG} ${VALID_TEST_TARGETS} ; then
                test_targets_queue $OPTARG
            else
                error "Invalid target $OPTARG"
            fi
            ;;
        r)
            [ ! -d $OPTARG ] && error "Can't find old test results in $OPTARG"
            RESULTS2COMPARE=$OPTARG
            ;;
        T)  TARGET_IPADDR=$OPTARG ;;
        s)  DIR_SOURCE=${OPTARG%/} ;;
        S)  SERIAL_DEBUG_PORT=true ;;
        H)  HOST_TARGET="--host=$OPTARG" ;;
        b)  DIR_BUILD=${OPTARG%/} ;;
        o)
            DIR_ELF_OUTPUT=$OPTARG-elf
            DIR_uC_OUTPUT=$OPTARG-uclinux
            DIR_LINUX_OUTPUT=$OPTARG-linux-uclibc
            ;;
        O)
            DIR_ELF_OUTPUT=$OPTARG
            DIR_uC_OUTPUT=$OPTARG
            DIR_LINUX_OUTPUT=$OPTARG
            ;;
        f)  STACK_CHECKING=true ;;
        L)  LEGACY_RESULT=1 ;;
        h)  usage ;;
        *)  error "unknown option $parameter\n" \
                  "Try \`$APP_NAME -h' for more information."
            ;;
    esac
done

shift $(($OPTIND - 1))

#
# Process all the tests to run.  They take the form:
# [!]<test>[=testsuite dir]
#

TESTS=
test_queue() { TESTS="${TESTS} $*" ; }
test_has()   { has $1 ${TESTS} ; }
test_remove()
{
	local t tests="${TESTS}"
	TESTS=
	for t in ${tests} ; do
		has $t "$@" || test_queue ${t}
	done
}
SKIP_TESTS=
test_skip_queue() { SKIP_TESTS="${SKIP_TESTS} $*" ; }
testsuite_var() { echo ${1}_TESTSUITE | tr '[:lower:]' '[:upper:]' | sed -e 's:[+]:X:g' ; }
testsuite_set() { eval `testsuite_var $1`=\"$2\" ; }
testsuite_get()
{
	local out="$(eval echo \$`testsuite_var $1`)"
	echo ${out:-$2}
}

for test in ${VALID_TESTS} ; do
	testsuite_set ${test}
done

for test in "$@" ; do
	case ${test} in
		!*) remove=true ;;
		*)  remove=false ;;
	esac
	test=${test#!}

	case ${test} in
		*=*) testsuite=${test#*=} ;;
		*)   testsuite= ;;
	esac
	test=${test%%=*}

	if ! has ${test} ${VALID_TESTS} ; then
		error "Unknown test: ${test}\nValid tests: " ${VALID_TESTS}
	fi

	if ${remove} ; then
		test_skip_queue ${test}
	else
		test_queue ${test}
	fi

	[ -n "${testsuite}" ] && testsuite_set ${test} "${testsuite}"
done
: ${TESTS:=${VALID_TESTS}}
test_remove ${SKIP_TESTS}

# Default to running the elf tests on the simulator, and the hw
# tests if a board target was specified
if [ -z "${TEST_TARGETS}" ] ; then
	test_targets_queue elf-sim
	[ $TARGET_IPADDR ] && test_targets_queue LINUX
fi

if test_targets_has LINUX ; then
	[ ! $TARGET_IPADDR ] && error "No target ip address provided"
fi

notice "Targets" ${TEST_TARGETS}
notice "Components" ${TESTS}

[ "${TESTS}" = "${VALID_TESTS}" ] && RESUME_BUILD=false || RESUME_BUILD=true

# I'd like to echo host triplet and build triplet here.
# But until now we have not checked validity of source paths.
# Maybe we need put a copy of config.guess and config.sub
# along with this build script.

#FIXME Check if HOST_ALIAS can be recognized by config.sub
#      Check if config.guess is successful

BUILD_TRIPLET=`$DIR_APP/config.guess`
if [ -z "$HOST_ALIAS" ] ; then
    HOST_TRIPLET=`$DIR_APP/config.guess`
else
    HOST_TRIPLET=`$DIR_APP/config.sub $HOST_ALIAS`
fi
BUILD_TARGET="--build=$BUILD_TRIPLET"
HOST_TARGET="--host=$HOST_TRIPLET"

notice "Host" "$HOST_TRIPLET"
notice "Build" "$BUILD_TRIPLET"

# If we want to test a toolchain in a specified path, make sure
# we will not test tools in other path.
if [ $DIR_ELF_OUTPUT ] ; then
	scrub_path
	PATH=$NEW_PATH:$DIR_ELF_OUTPUT/bin:$DIR_uC_OUTPUT/bin:$DIR_LINUX_OUTPUT/bin
fi

############ Check and error if I can't find the things I need #


PREREQ_FILES="${PREREQ_FILE} ${RUNTEST} ${PREREQ_TESTS}"

if test_targets_has LINUX ; then
	PREREQ_FILES="${PREREQ_FILES} ${PREREQ_HW_TESTS}"
fi

if [ -n "$CHECK_PRE" ] ; then
	check_prereqs_verbose ${PREREQ_FILES}
	exit 0
else
	check_prereqs_short ${PREREQ_FILES}
fi

############ ABSOLUTE PATH TO SOURCE DIRECTORIES ################

# use the saved results by default
if [ -z "${DATE_STAMP}${RESULTS2COMPARE}" ] ; then
	[ -d test_results ] && RESULTS2COMPARE="test_results"
fi
# let people use relative paths to old test logs
case ${RESULTS2COMPARE} in /*) ;; *) RESULTS2COMPARE="${PWD}/${RESULTS2COMPARE}" ;; esac

check_abs_dir "Toolchain source" "${DIR_SOURCE}"

: ${DIR_BUILD:=${PWD}}

mkdir -p "${DIR_BUILD}"
check_abs_dir "" "${DIR_BUILD}"

# make sure we arent running on a case insensitive filesystem
cd "${DIR_BUILD}"
check_fs_case

DIR_LOG=$DIR_BUILD/logs
mk_output_dir "logs" "${DIR_LOG}"

DIR_TEST_RESULTS=$DIR_BUILD/tests
mk_output_dir "tests" "${DIR_TEST_RESULTS}" "don't clean it"

get_results_dir() { echo ${DIR_TEST_RESULTS}/$1${DATE_STAMP+-${TIMESTAMP}} ; }
for tt in ${TEST_TARGETS} ; do
	results=$(get_results_dir ${tt})
	mk_output_dir "${tt}" "${results}"
	if [ -n "${RESULTS2COMPARE}" ] ; then
		d="${RESULTS2COMPARE}/${tt}"
		is_abs_dir "${d}" && m= || m=" (!!missing!!)" 
		notice "Previous ${tt} test logs${m}" "${d}"
	fi
done

###################### set up log file ###########################

STDOUT_LOG="$DIR_LOG/log${DATE_STAMP+.${TIMESTAMP}}"
STDERR_LOG="$DIR_LOG/warn${DATE_STAMP+.${TIMESTAMP}}"
notice "Creating log file" "${STDOUT_LOG}"
if [ $VERBOSE ]; then
    LOGFILE="| tee -a ${STDOUT_LOG}"
    ACTUAL_LOGFILE=${STDOUT_LOG}
else
    if [ $WARN ] ; then
        echo "*** Warnings going to $DIR_LOG/warn"
        LOGFILE=">> ${STDOUT_LOG} 2>> ${STDERR_LOG}"
        ACTUAL_LOGFILE=${STDERR_LOG}
    else
        LOGFILE=">> ${STDOUT_LOG} 2>&1"
        ACTUAL_LOGFILE=${STDOUT_LOG}
    fi
fi


kill_procs()
{
	local procs
	local uname
	uname=`id -un`
	procs=$(eval `printf 'pgrep -u $uname "%s";' "$@"`)
	if [ -n "${procs}" ] ; then
		echo_date "Found old processes ($*); killing them to avoid confusing the testsuites"
		run_cmd ps u ${procs}
		run_cmd kill ${procs}
	fi
}

dorsh()
{
	# rsh sucks and does not pass up its exit status, so we have to:
	#  on board:
	#    - send all output to stdout
	#    - send exit status to stderr
	#  on host:
	#    - swap stdout and stderr
	#    - pass exit status to `exit`
	#    - send stderr back to stdout and up
	(exit \
		$(rsh -l root ${TARGET_IPADDR} "$@" 3>&1 1>&2 2>&3 </dev/null) \
		2>&1) 2>&1
}
run_rsh_nodie_stdout()
{
	log_it rsh -l root ${TARGET_IPADDR} "$@"
	dorsh "$@"
}
run_rsh_nodie()
{
	eval \($(requote run_rsh_nodie_stdout "$@")\) ${LOGFILE}
}
run_rcp()
{
	set -- rcp root@${TARGET_IPADDR}:"$@"
	run_cmd "$@"
}
log_on_board()
{
	run_rsh_nodie "echo >/dev/console; echo $* >/dev/console"
}

# stop if there are any Xvfb or rsh processes, which might be lurking around from old runs
kill_procs Xvfb

if test_targets_has LINUX ; then
	# stop if there are any rsh processes, which might be lurking around from old runs
	kill_procs rsh

	# Check if target board is available using ping, rcp and rsh.
	run_cmd ping -n -c 1 ${TARGET_IPADDR}
	if ! run_rsh_nodie ls / ; then
		error "Can't rsh to $TARGET_IPADDR, check rsh client on host, and inetd/rshd on target"
	fi

	# See if the board is running FDPIC or FLAT
	if ! RUN=`run_rsh_nodie_stdout find / -name gdbserver | head -1` ; then
		error "unable to find gdbserver!"
	else 
		run_rcp $RUN "${DIR_TEST_RESULTS}"/gdbserver
		FDPIC_TARGET=0
		if file "${DIR_TEST_RESULTS}"/gdbserver | grep -q ELF ; then
			FDPIC_TARGET=1
		elif run_rsh_nodie_stdout cat /proc/maps | grep -q /lib/ ; then
			FDPIC_TARGET=1
		fi
		run_cmd rm "${DIR_TEST_RESULTS}"/gdbserver
	fi
	# Check if the tcp connections are recycled on both target
	# and host, which is needed to prevent the rsh and rcp
	# connection failures when doing many many connections.

	sysctl="/proc/sys/net/ipv4/tcp_tw_recycle"

	notice "Checking host sysctl" "${sysctl}"
	RUN=`cat ${sysctl} 2>/dev/null`
	[ -z "$RUN"    ] && error "unable to read ${sysctl}; check host kernel settings"
	[ "$RUN" = "0" ] && error "please run: sudo sh -c 'echo 1 > ${sysctl}'"

	notice "Checking target sysctl" "${sysctl}"
	if ! RUN=`run_rsh_nodie_stdout cat ${sysctl}` ; then
		error "unable to read ${sysctl}; check Blackfin kernel settings"
	fi
	if [ "$RUN" = "0" ] ; then
		run_rsh_nodie "echo 1 > ${sysctl}"
		RUN=`run_rsh_nodie_stdout cat ${sysctl}`
		if [ "$RUN" != "1" ] ; then
			error "could not set ${sysctl} to 1"
		fi
	fi

	# Check out the target CPU
	if ! RUN=`run_rsh_nodie_stdout cat /proc/cpuinfo` ; then
		error "unable to read Blackfin /proc/cpuinfo"
	fi
	if ! echo "${RUN}" | grep -q 'ADI' ; then
		error "board doesn't appear to be an ADI chip "
	fi
	CPU_TYPE=`echo "${RUN}" | awk '$1 == "model" { print $4; exit }' | tr '[:upper:]' '[:lower:]' | sed 's:adsp-::'`
	CPU_REV=`echo "${RUN}" | awk '$1 == "stepping" { print $3; exit }'`
	CPU_TYPE_REV="${CPU_TYPE}-0.${CPU_REV}"
	notice "Detected board cpu" "${CPU_TYPE_REV}"

	if [ $STACK_CHECKING ] ; then
		echo "    Hardware tests will be compiled with optional stack overflow checking"
	fi
	if test_targets_has linux-uclibc ; then
		if [ $FDPIC_TARGET -eq 0 ] ; then
			echo  "    Looks like flat userspace - I'll check /lib/*.so are the same when testing"
		else
			error "    Your Target looks like it is running FDPIC userspace - I can't test on fdpic targets. Stopping"
		fi
	fi

	# Make sure syslog doesnt grab debug messages (kernel dumps)
	run_rcp /var/log/messages "${DIR_TEST_RESULTS}"/messages.boot
	run_rsh_nodie '> /var/log/messages'
	if run_rsh_nodie_stdout ps | grep syslog | grep -q -v -e '-l 5' ; then
		echo "    Board at '${TARGET_IPADDR}' being set so syslog doesn't fill /var/log/messages"
		run_rsh_nodie sed -i '"/^slog:/s:$: -l 5:"' /etc/inittab
	fi
	run_rsh_nodie kill -HUP 1

	run_rsh_nodie dmesg -n 5
fi

############ Done checking - lets get on to testing ###################

get_tuple()
{
	case $1 in
		elf*)          echo bfin-elf ;;
		uclinux*)      echo bfin-uclinux ;;
		linux-uclibc*) echo bfin-linux-uclibc ;;
	esac
}

# Some of the tests leave stuff all over the target file system,
# by getting rid of it, we free up lots of memory.
clean_hardware()
{
	# only clean targets running under Linux
	test_targets_has LINUX || return 0

	run_rsh_nodie 'free; find / /tmp/ /root/ -maxdepth 1 -type f -name "[^.]*" -exec rm "{}" \; ; free'

	# clean up any servers laying around
	run_rsh_nodie killall gdbserver

	# we dump something onto the console so we can track which application caused a crash
	log_on_board testing $1 $2
}

copy_test_results()
{
	local sum name target
	for sum in `find ./ -name "*.sum"` ; do
		sum=${sum%.sum}
		name=${sum##*/}
		# flatten the subdir tree into a filename
		target=`echo $sum | sed -e 's:^\./::' -e 's:/testsuite/:/:g' -e 's:/:-:g'`
		[ "${target}" = "${name}" ] && target=
		target=${name}-${2}${target:+-${target}}
		run_cmd cp ${sum}.sum $1/${target}.sum
		run_cmd cp ${sum}.log $1/${target}.log
	done
}

# Get the target hardware ready for testing:
#  - check we can access it
#  - set syslogd & tcp_tw_recycle
#  - copy over all the shared libs to the target /lib dir (for fdpic tests, and if flat userspace)
# prepare_target test_target_dir
prepare_target()
{
	# only clean targets running Linux
	test_targets_has LINUX || return 0

	log_it "preparing target board for more tests"

	# Check if target board is available using ping
	local count=1
	while ! run_cmd ping -n -c 1 ${TARGET_IPADDR} ; do
		: $(( count += 1 ))
		[ ${count} -gt 10 ] && error "could not ping target board - check network settings"
		sleep 30
	done

	# Just in case we caused the kernel under test to crash, let's do a sanity test
	# make sure syslogd is set to not log rsh/rcp as it will fill /var/log/messages
	if run_rsh_nodie_stdout ps | grep syslog | grep -q -v -e '-l 5' ; then
		log_on_board "Setting syslogd"
		run_rsh_nodie '> /var/log/messages'
		run_rsh_nodie sed -i '"/^slog:/s:$: -l 5:"' /etc/inittab
		run_rsh_nodie kill -HUP 1
	fi

	# see sysctl logic in initial code above
	run_rsh_nodie "echo 1 > /proc/sys/net/ipv4/tcp_tw_recycle"
}

# A very light check_tests_OK, which only check if the target board is hung.
# Let runtest timeout handle host side.
# Note: we cannot write to the log file here as the tests are running parallel.
check_tests_OK()
{
	local pid=$1

	if test_targets_has LINUX ; then
		local timeout=0 limit=3
		while :; do
			# Check on the Host - if the test is done - quit
			if ! ps -p $pid >/dev/null 2>&1 ; then
				return 0
			fi

			# make sure the board is not hung
			if ! dorsh ls / >/dev/null 2>&1 ; then
				: $(( timeout += 1 ))
			else
				timeout=0
			fi

			if [ ${timeout} -gt ${limit} ] ; then
				echo "*** I think the board or network died during testing - sorry "
				echo "*** Kill the current testing"
				run_cmd_nodie kill $pid
				return 1
			fi
			sleep 5
		done

	else
		wait
	fi
}

detect()
{
	local pkg t ver path tool_alias tool
	pkg=$1
	t=$2
	ver=$3
	path=$4

	tool_alias=$target_alias-$t
	tool=$(which $tool_alias 2>/dev/null)
	if [ -z "$tool" ] ; then
		echo "$tool_alias not found"
		return 1
	fi

	VERSION=`${tool} --version 2>&1 | sed -e 's/([^)]*)//g' | awk '{print $NF; exit;}'`
	if [ -z "${ver}" ] ; then
		notice "$pkg version" "$VERSION"
	elif [ "${ver}" != "$VERSION" ] ; then
		echo "ERROR: $tool_alias version ($VERSION) is different from source version ($ver)"
		return 1
	fi

	TOOL_PATH=${tool%/*}
	if [ -z "$path" ] ; then
		notice "$pkg path" "$TOOL_PATH"
	elif [ "$path" != "$TOOL_PATH" ] ; then
		echo "ERROR: $tool_alias path ($TOOL_PATH) is different from $path"
		return 1
	fi

	return 0
}

make_site_exp()
{
	log_it "Using testsuite in ${test_src}"
	cat <<-EOF > "${test_output}"/site.exp
	set host_triplet $HOST_TRIPLET
	set build_triplet $BUILD_TRIPLET
	set target_triplet $TARGET_TRIPLET
	set target_alias $target_alias
	set tmpdir ${test_output}
	set srcdir ${test_src}
	$(printf '%s\n' "$@")
	set tool_version ""
	set HOSTCC gcc
	set HOSTCFLAGS ""
	EOF
	run_cmd cat "${test_output}"/site.exp
}

# test_common <pkg-name> <testsuite-src> <version> <result-dir>
test_hook() { :; }
test_common()
{
	local test_output pkg result_dir test_src ver
	pkg=$1
	test_src=`testsuite_get ${pkg} "$2"`
	ver=$3
	result_dir=$4

	echo_date "Running tests on ${pkg}"

	if ! is_abs_dir "${test_src}" ; then
		echo "No testsuite for ${pkg} ${ver} in ${test_src}"
		return 1
	fi

	test_output=$DIR_BUILD/${pkg}_build/testsuite
	change_clean_dir "${test_output}"

	test_hook "${test_src}" "${test_output}"

	make_site_exp

	run_cmd_nodie $RUNTEST --tool ${pkg} $RUNTEST_FLAGS &
	check_tests_OK $! ${pkg} "${result_dir}"
	copy_test_results "${result_dir}" ${ver}

	clean_dir "$DIR_BUILD/${pkg}_build"

	test_hook() { :; }
}

# Detect Binutils version and path
# $1 is the name of the tool

detect_binutils()
{
	for t in "$@" ; do
		detect binutils $t "${BINUTILS_VERSION}" "${BINUTILS_PATH}" || return 1
		: ${BINUTILS_VERSION:=${VERSION}}
		: ${DIR_BINUTILS_SOURCE:=$DIR_SOURCE/binutils-$BINUTILS_VERSION}
		if ! is_abs_dir "$DIR_BINUTILS_SOURCE" ; then
			echo "Can't find Binutils source at $DIR_BINUTILS_SOURCE"
			return 1
		fi
		: ${DIR_BINUTILS_BUILD=$DIR_BUILD/binutils_build}
		: ${BINUTILS_PATH:=${TOOL_PATH}}
	done
}

# test_common_binutils <package-name> <results-dir> <utils-to-test-for>
test_common_binutils()
{
	detect_binutils $3 || return 1
	test_common $1 \
		"$DIR_BINUTILS_SOURCE/$1/testsuite" \
		"${BINUTILS_VERSION}" \
		"$2"
}
test_binutils() { test_common_binutils "$@" "ar nm objcopy objdump readelf size" ; }
test_gas()      { test_common_binutils "$@" "as" ; }
test_ld()       { test_common_binutils "$@" "ld" ; }

# Detect GDB version and path
# $1 is the name of gdb tool

detect_gdb()
{
	detect gdb $1 "$GDB_VERSION" "$GDB_PATH" || return 1
	: ${GDB_VERSION:=${VERSION}}
	: ${GDB_PATH:=${TOOL_PATH}}

	# figure out which binutils dir this gdb ver is coming from
	if [ "${DIR_GDB_SOURCE:+set}" != "set" ] ; then
		for d in "$DIR_SOURCE"/binutils-*/gdb ; do
			if [ "$(cat "${d}"/version.in)" = "${GDB_VERSION}" ] ; then
				DIR_GDB_SOURCE=${d%/gdb}
				break
			fi
		done
	fi
	if ! is_abs_dir "$DIR_GDB_SOURCE" ; then
		echo "Can't find GDB source at $DIR_GDB_SOURCE"
		return 1
	fi
}

test_common_gdb()
{
	detect_gdb gdb || return 1
	test_hook()
	{
		# gdb testsuite generates binaries in subdirectories, so to test
		# the installed gdb, we have to establish the same directory
		# structure in objdir as is in the testsuite source dir.
		(
		cd "$1"
		find -type d ! -path *.svn* -exec mkdir -p "$2"/\{\} \;
		)
	}

	test_common $1 \
		"$DIR_GDB_SOURCE/$3/testsuite" \
		"${GDB_VERSION}" \
		"$2"
}

test_gdb() { test_common_gdb "$@" "gdb" ; }
# would be better to detect `run`, but it doesn't have proper version output.
# while the sim isn't strictly tied to gdb, it's how upstream organizes.
test_sim() { test_common_gdb "$@" "sim" ; }

# Detect version and path for objc

detect_objc()
{
	local tool_alias
	detect_gcc gcc || return 1
	tool_alias=$target_alias-gcc

	echo "
#include <objc/Object.h>

@interface Foo:Object
{
}
- (void)foo;
@end

@implementation Foo
- (void)foo
{
}
@end

int main(void)
{
  id myFoo = [Foo new];
  [myFoo free];
  return 0;
}
" | ${tool_alias} -x objective-c - -lobjc -o /dev/null 2>/dev/null 1>&2

	if [ $? -ne 0 ] ; then
		echo "objc support not found in $target_alias toolchain"
		return 1
	else
		return 0
	fi
}

# Detect version and path for gcc tool
# $1 is the name of tool: gcc, g++, gfortran

detect_gcc()
{
	for t in "$@" ; do
		detect gcc $t "${GCC_VERSION}" "${GCC_PATH}" || return 1
		: ${GCC_VERSION:=${VERSION}}
		: ${DIR_GCC_SOURCE:=$DIR_SOURCE/gcc-${GCC_VERSION%.?}}
		if ! is_abs_dir "$DIR_GCC_SOURCE" ; then
			echo "Can't find gcc source at $DIR_GCC_SOURCE"
			return 1
		fi
		: ${DIR_GCC_BUILD=$DIR_BUILD/gcc-${GCC_VERSION}_build}
		: ${GCC_PATH:=${TOOL_PATH}}
	done
}

# test_common_gcc <pkg-name> <result-dir> [utils-to-test-for] [pkg-subdir]
test_common_gcc()
{
	detect_gcc ${3:-$1} || return 1
	local default_test_src p
	for p in "$4" "$1" "gcc" ; do
		default_test_src="$DIR_GCC_SOURCE/$p/testsuite"
		[ -d "${default_test_src}" ] && break
	done
	test_common $1 \
		"${default_test_src}" \
		"${GCC_VERSION%.?}" \
		"$2"
}
test_gcc()       { test_common_gcc "$@" ; }
test_gxx()       { test_common_gcc "$@" ; }
test_gfortran()  { test_common_gcc "$@" ; }
test_libstdcxx() { test_common_gcc "$@" "g++" libstdc++-v3 ; }
test_libgomp()   { test_common_gcc "$@" "gcc g++" ; }
test_objc()
{
	detect_objc || return 1
	test_common_gcc "$@" "gcc"
}

test_newlib()
{
	detect_gcc gcc || return 1
	test_common $1 \
		"$DIR_BINUTILS_SOURCE/$1/testsuite" \
		"${GCC_VERSION%.?}" \
		"$2"
}

test_newlib_multilib()
{
	local test_output pkg result_dir test_src ver
	pkg=$1
	result_dir=$2

	detect_binutils as || return 1
	detect_gcc gcc || return 1
	ver=${GCC_VERSION%.?}
	test_src=`testsuite_get ${pkg} "$DIR_BINUTILS_SOURCE/newlib/testsuite"`

	if ! is_abs_dir "${test_src}" ; then
		echo "No testsuite for ${pkg} ${ver} in ${test_src}"
		return 1
	fi

	for ml in `$target_alias-gcc --print-multi-lib` ; do
		dir=`echo ${ml} | sed -e 's/;.*$//'`
		flags=`echo ${ml} | sed -e 's/^[^;]*;//' -e 's/@/ -/g'`

		echo_date "Running tests on ${pkg} (${dir})"

		test_output=$DIR_GCC_BUILD/${pkg}/testsuite/${dir}
		change_clean_dir "${test_output%.}"

		make_site_exp "set CFLAGS_FOR_TARGET \"$flags\""

		run_cmd_nodie $RUNTEST --tool ${pkg} $RUNTEST_FLAGS &
		check_tests_OK $! ${pkg} "${result_dir}"
	done

	change_dir "$DIR_GCC_BUILD/${pkg}/testsuite"
	copy_test_results "${result_dir}" ${ver}

	clean_dir "${DIR_GCC_BUILD}"
}

test_libmudflap()
{
	test_hook()
	{
		local test_src="$1" test_output="$2"
		case ${test_target} in
			uclinux*)      build_libmudflapth=1 LIBS="" ;;
			linux-uclibc*) build_libmudflapth=1 LIBS="-ldl" ;;
			*)             build_libmudflapth=0 LIBS="" ;;
		esac
		sed \
			-e "s/@LIBS@/$LIBS/g" \
			-e "s/@build_libmudflapth@/$build_libmudflapth/g" \
			$test_src/mfconfig.exp.in > $test_output/mfconfig.exp

		# Some tests expect there is a config.h in ../.. directory
		cat <<-EOF > $DIR_BUILD/config.h
		#define HAVE_SYS_MMAN_H	1
		#define HAVE_MMAP	1
		EOF
	}
	test_common_gcc "$@" "gcc g++"
}

prepare_target_solibs()
{
	# Make sure all the shared libs are actually on the target and are
	# the same if we are testing on the hardware.

	# TODO
	#
	# * Check if linux kernel has fdpic feature enabled.
	# * Check if there are shared libraries installed on target. (Try
	#   to run a minimal program to see if it can run on target.)
	# * Check if the shared libraries on target are same as those in
	#   toolchain. (We may need --build-id from latest GNU ld.)

	# Current only copy shared libraries from toolchain to target
	# when safe. That means only a flat image can be used for testing
	# linux-uclibc.

	[ $FDPIC_TARGET -ne 0 ] && return 0

	detect_gcc gcc || return 1

	echo_date "Checking shared libraries on target"

	GCC_BUILD=$DIR_BUILD/gcc_build-$GCC_VERSION
	run_cmd mkdir -p $GCC_BUILD/libs_for_target
	run_cmd rm -f $GCC_BUILD/libs_for_target/*
	t=`$target_alias-gcc -mcpu=${CPU_TYPE_REV} -print-file-name=libc.a`
	t=`dirname $t`/../..
	for FILE in $t/lib/*so* ; do
		if [ -e $FILE ] && [ -h $FILE ] ; then
			# don't check sym links
			LIB=`readlink $FILE`
			FILE=`basename $FILE`
			LIB=`basename $LIB`
			run_rsh_nodie "ln -s $LIB /lib/$FILE"
		else
			if [ -f $FILE ] ; then
				LIB=`basename ${FILE}`
				run_cmd $target_alias-strip $FILE -o $GCC_BUILD/libs_for_target/${LIB}
				BOARD_LIB=`run_rsh_nodie_stdout md5sum /lib/${LIB} | awk '{print $1}'`
				TEST_LIB=`md5sum $GCC_BUILD/libs_for_target/${LIB} | awk '{print $1}'`
				if [ "$BOARD_LIB" != "$TEST_LIB" ] ; then
					error "Libs on board are different (${LIB} and ${FILE}), aborting tests"
				fi
			fi
		fi
	done
	run_cmd rm -rf $GCC_BUILD/libs_for_target
}

# run_tests $directory_test_results_go_in name_of_test
run_tests()
{
	local test_target=$1
	local result_dir="$(get_results_dir ${test_target})"
	local target_alias=$(get_tuple ${test_target})

	local hardware
	case ${test_target} in
		*-sim)  hardware="simulator" ;;
		*-qemu) hardware="qemu" ;;
		*-jtag) hardware="jtag" ;;
		*)      hardware="hardware (${TARGET_IPADDR})" ;;
	esac
	echo_date "Testing ${target_alias} via ${hardware}"

	TARGET_TRIPLET=`$DIR_APP/config.sub $target_alias`

	export ADI_TARGET_LIST="bfin-${test_target}"
	prepare_target ${test_target}

	tt="binutils gas ld gdb gcc g++ gfortran objc libstdc++"
	case ${test_target} in
		elf-sim)       tt="${tt} sim" ;;
		elf-jtag)      tt="${tt} newlib" ;;
		uclinux*)      tt="${tt} libmudflap libgomp" ;;
		linux-uclibc*) tt="${tt} libmudflap libgomp"
		               [ -n "$CHECK_LIB" ] && prepare_target_solibs
		               ;;
	esac
	for t in ${tt} ; do
		test_has ${t} || continue

		kill_procs Xvfb rsh
		clean_hardware ${test_target} ${t}
		test_`echo ${t} | sed 's:[+]:x:g'` ${t} "${result_dir}"
	done

	if test_has newlib ; then
		if [ ${test_target} = "elf-sim" ] ; then
			test_newlib_multilib newlib ${result_dir}
			# test_newlib ${result_dir}
		fi
	fi

	clean_hardware ${test_target} done

    if [ $LEGACY_RESULT ] ; then
	echo_date "Legacify result names"

	for i in `find ${result_dir} -name "*newlib-$GCC_VERSION.*" -o -name "*libstdc++-$GCC_VERSION.*" -o -name "*libmudflap-$GCC_VERSION.*" -o -name "*libgomp-$GCC_VERSION.*" `
	do
	    p=`dirname $i`
	    n=`basename $i`
	    case "$n" in
		$target_alias-*)
		    continue
		    ;;
		*)
		    mv $i $p/$target_alias-$n
		    ;;
	    esac
	done

	for i in `find ${result_dir} -name "g++-$GCC_VERSION.*" -o -name "gfortran-$GCC_VERSION.*" -o -name "objc-$GCC_VERSION.*"`
	do
	    p=`dirname $i`
	    n=`basename $i`
	    mv $i $p/gcc-$n
	done
    fi

	# Leave the failed testcases and delete the passed one.
	find "$DIR_BUILD" -name '*.gdb' -type f | \
	while read testcase ; do
		[ -e "${testcase%.gdb}" ] && rm -f "$testcase"
	done

	echo_date "Done testing - results"

	local compare_summary="${result_dir}/compare_summary"
	echo "### Old compare sumary is in: ${compare_summary}"  > ${compare_summary}

	for new_sum in $(find "${result_dir}" -name '*.sum' | sort)
	do
		# clean up the log to make manual `diff` easier
		sed -i \
			-e "s: ${DIR_SOURCE%/}/: :g" \
			"${new_sum}"

		n=${new_sum##*/}
		echo
		echo "### New: ${n} in ${new_sum%/*}"
		egrep "=== .*Sum.* ===|^# of" $new_sum

		(
		if [ -n "$RESULTS2COMPARE" ] ; then
			old_sum=$RESULTS2COMPARE/${test_target}/$n
			if [ -f $old_sum ] ; then
				echo
				echo "### Compared with the old: ${old_sum##*/} in ${old_sum%/*}"
				`printf '%s\n' "$DIR_SOURCE"/gcc-*/contrib/compare_tests | tail -n 1` \
					"$old_sum" "$new_sum"
			else
				echo "Could not find $old_sum to compare"
			fi
		fi
		) >> "${compare_summary}" 2>&1
	done
	echo
}

###################### Setting up test  ##########################

mk_output_dir "boards" "${DIR_BUILD}/boards" "don't clean"

#
# Set up global site.exp
#

STACK_CHECKING_SET=
if [ $STACK_CHECKING ] ; then
	STACK_CHECKING_SET='set_board_info cflags " -mstack-check-l1 "'
fi
SERIAL_DEBUG_PORT_SET=
if [ $SERIAL_DEBUG_PORT ] ; then
	SERIAL_DEBUG_PORT_SET='set_board_info use_serial_debug 1\nset GDBFLAGS "-nx -b 57600"'
fi

: ${CPU_TYPE_REV:=bf537-0.2}
CPU_TYPE_REV="-mcpu=${CPU_TYPE_REV}"

export DEJAGNU=$DIR_BUILD/boards/site.exp
(
sed \
	-e "s:@BOARDS_DIR@:${DIR_BOARDS}:" \
	-e "s:@BOARDS_BUILD_DIR@:$DIR_BUILD/boards:" \
	-e "s:@MCPU_FLAGS@:${CPU_TYPE_REV} ${STACK_CHECKING:+-mstack-check-l1}:" \
	-e "s:@HOSTNAME@:$TARGET_IPADDR:" \
	"${DIR_BOARDS}"/site.exp.in
`printf "$SERIAL_DEBUG_PORT_SET\n"`
) > "${DEJAGNU}"

#
# Finally run the actual tests
#
for test_target in ${TEST_TARGETS} ; do
	run_tests ${test_target}
done
echo_date "It took $(print_stop_time) to complete"
